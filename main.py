import requests
import streamlit as st
from streamlit_lottie import st_lottie
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Page Config
st.set_page_config(
    page_title="ë…¸ë˜ ê°€ì‚¬ ní–‰ì‹œ",
    page_icon="ğŸ’Œ",
    layout="wide"
)

### Model
tokenizer = AutoTokenizer.from_pretrained("wumusill/final_project_kogpt2")

@st.cache(show_spinner=False)
def load_model():
    model = AutoModelForCausalLM.from_pretrained("wumusill/final_project_kogpt2")
    return model

model = load_model()

# Class : Dict ì¤‘ë³µ í‚¤ ì¶œë ¥
class poem(object):
    def __init__(self,letter):
        self.letter = letter

    def __str__(self):
        return self.letter

    def __repr__(self):
        return "'"+self.letter+"'"


def n_line_poem(input_letter):

    # ë‘ìŒ ë²•ì¹™ ì‚¬ì „
    dooeum = {"ë¼":"ë‚˜", "ë½":"ë‚™", "ë€":"ë‚œ", "ë„":"ë‚ ", "ëŒ":"ë‚¨", "ë":"ë‚©", "ë‘":"ë‚­", 
          "ë˜":"ë‚´", "ë­":"ëƒ‰", "ëƒ‘":"ì•½", "ëµ":"ì•½", "ëƒ¥":"ì–‘", "ëŸ‰":"ì–‘", "ë…€":"ì—¬", 
          "ë ¤":"ì—¬", "ë…":"ì—­", "ë ¥":"ì—­", "ë…„":"ì—°", "ë ¨":"ì—°", "ë…ˆ":"ì—´", "ë ¬":"ì—´", 
          "ë…":"ì—¼", "ë ´":"ì—¼", "ë µ":"ì—½", "ë…•":"ì˜", "ë ¹":"ì˜", "ë…œ":"ì˜ˆ", "ë¡€":"ì˜ˆ", 
          "ë¡œ":"ë…¸", "ë¡":"ë…¹", "ë¡ ":"ë…¼", "ë¡±":"ë†", "ë¢°":"ë‡Œ", "ë‡¨":"ìš”", "ë£Œ":"ìš”", 
          "ë£¡":"ìš©", "ë£¨":"ëˆ„", "ë‰´":"ìœ ", "ë¥˜":"ìœ ", "ë‰µ":"ìœ¡", "ë¥™":"ìœ¡", "ë¥œ":"ìœ¤", 
          "ë¥ ":"ìœ¨", "ë¥­":"ìœµ", "ë¥µ":"ëŠ‘", "ë¦„":"ëŠ ", "ë¦‰":"ëŠ¥", "ë‹ˆ":"ì´", "ë¦¬":"ì´", 
          "ë¦°":'ì¸', 'ë¦¼':'ì„', 'ë¦½':'ì…'}
    # ê²°ê³¼ë¬¼ì„ ë‹´ì„ list
    res_l = []

    # í•œ ê¸€ìì”© ì¸ë±ìŠ¤ì™€ í•¨ê»˜ ê°€ì ¸ì˜´
    for idx, val in enumerate(input_letter):
        # ë‘ìŒ ë²•ì¹™ ì ìš©
        if val in dooeum.keys():
            val = dooeum[val]


        while True:
            # ë§Œì•½ idx ê°€ 0 ì´ë¼ë©´ == ì²« ê¸€ì
            if idx == 0:
                # ì²« ê¸€ì ì¸ì½”ë”©
                input_ids = tokenizer.encode(
                val, add_special_tokens=False, return_tensors="pt")
                # print(f"{idx}ë²ˆ ì¸ì½”ë”© : {input_ids}\n") # 2ì°¨ì› í…ì„œ

                # ì²« ê¸€ì ì¸ì½”ë”© ê°’ìœ¼ë¡œ ë¬¸ì¥ ìƒì„±
                output_sequence = model.generate(
                    input_ids=input_ids, 
                    do_sample=True, max_length=42,
                    min_length=5, temperature=0.9, repetition_penalty=1.5,
                    no_repeat_ngram_size=2)[0]
                # print("ì²« ê¸€ì ì¸ì½”ë”© í›„ generate ê²°ê³¼:", output_sequence, "\n") # tensor

            # ì²« ê¸€ìê°€ ì•„ë‹ˆë¼ë©´
            else:
                # í•œ ìŒì ˆ
                input_ids = tokenizer.encode(
                val, add_special_tokens=False, return_tensors="pt")
                # print(f"{idx}ë²ˆ ì§¸ ê¸€ì ì¸ì½”ë”© : {input_ids} \n")

                # ì¢€ë” ë§¤ë„ëŸ¬ìš´ ì‚¼í–‰ì‹œë¥¼ ìœ„í•´ ì´ì „ ì¸ì½”ë”©ê³¼ ì§€ê¸ˆ ì¸ì½”ë”© ì—°ê²°
                link_with_pre_sentence = torch.cat((generated_sequence, input_ids[0]), 0)
                link_with_pre_sentence = torch.reshape(link_with_pre_sentence, (1, len(link_with_pre_sentence)))
                # print(f"ì´ì „ í…ì„œì™€ ì—°ê²°ëœ í…ì„œ {link_with_pre_sentence} \n")

                # ì¸ì½”ë”© ê°’ìœ¼ë¡œ ë¬¸ì¥ ìƒì„±
                output_sequence = model.generate(
                    input_ids=link_with_pre_sentence, 
                    do_sample=True, max_length=42,
                    min_length=5, temperature=0.9, repetition_penalty=1.5,
                    no_repeat_ngram_size=2)[0]
                # print(f"{idx}ë²ˆ ì¸ì½”ë”© í›„ generate : {output_sequence}")
        
            # ìƒì„±ëœ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì¸ì½”ë”© ë˜ì–´ìˆê³ , ìƒì„±ëœ ë¬¸ì¥ ë’¤ë¡œ padding ì´ ìˆëŠ” ìƒíƒœ)
            generated_sequence = output_sequence.tolist()
            # print(f"{idx}ë²ˆ ì¸ì½”ë”© ë¦¬ìŠ¤íŠ¸ : {generated_sequence} \n")

            # padding index ì•ê¹Œì§€ slicing í•¨ìœ¼ë¡œì¨ padding ì œê±°, paddingì´ ì—†ì„ ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì— ì¡°ê±´ë¬¸ í™•ì¸ í›„ ì œê±°
            if tokenizer.pad_token_id in generated_sequence:
                generated_sequence = generated_sequence[:generated_sequence.index(tokenizer.pad_token_id)]
            
            generated_sequence = torch.tensor(generated_sequence) 
            # print(f"{idx}ë²ˆ ì¸ì½”ë”© ë¦¬ìŠ¤íŠ¸ íŒ¨ë”© ì œê±° í›„ ë‹¤ì‹œ í…ì„œ : {generated_sequence} \n")

            # ì²« ê¸€ìê°€ ì•„ë‹ˆë¼ë©´, generate ëœ ìŒì ˆë§Œ ê²°ê³¼ë¬¼ listì— ë“¤ì–´ê°ˆ ìˆ˜ ìˆê²Œ ì• ë¬¸ì¥ì— ëŒ€í•œ ì¸ì½”ë”© ê°’ ì œê±°
            # print(generated_sequence)
            if idx != 0:
                # ì´ì „ ë¬¸ì¥ì˜ ê¸¸ì´ ì´í›„ë¡œ ìŠ¬ë¼ì´ì‹±í•´ì„œ ì• ë¬¸ì¥ ì œê±°
                generated_sequence = generated_sequence[len_sequence:]

            len_sequence = len(generated_sequence)
            # print("len_seq", len_sequence)

            # ìŒì ˆ ê·¸ëŒ€ë¡œ ë±‰ìœ¼ë©´ ë‹¤ì‹œ í•´ì™€, ì•„ë‹ˆë©´ whileë¬¸ íƒˆì¶œ
            if len_sequence > 1:
                break

        # ê²°ê³¼ë¬¼ ë¦¬ìŠ¤íŠ¸ì— ë‹´ê¸°
        res_l.append(generated_sequence)

    poem_dict = {}

    for letter, res in zip(input_letter, res_l):
        decode_res = tokenizer.decode(res, clean_up_tokenization_spaces=True, skip_special_tokens=True)
        poem_dict[poem(letter)] = decode_res

    return poem_dict

###

# Image(.gif)
@st.cache(show_spinner=False)
def load_lottieurl(url: str):
    r = requests.get(url)
    if r.status_code != 200:
        return None
    return r.json()

lottie_url = "https://assets7.lottiefiles.com/private_files/lf30_fjln45y5.json"

lottie_json = load_lottieurl(lottie_url)
st_lottie(lottie_json, speed=1, height=200, key="initial")


# Title
row0_spacer1, row0_1, row0_spacer2, row0_2, row0_spacer3 = st.columns(
    (0.01, 2, 0.05, 0.5, 0.01)
)

with row0_1:
    st.markdown("# í•œê¸€ ë…¸ë˜ ê°€ì‚¬ ní–‰ì‹œâœ")
    st.markdown("### ğŸ¦ë©‹ìŸì´ì‚¬ìì²˜ëŸ¼ AIS7ğŸ¦ - íŒŒì´ë„ í”„ë¡œì íŠ¸")

with row0_2:
    st.write("")
    st.write("")
    st.write("")
    st.subheader("1ì¡° - í•´íŒŒë¦¬")
    st.write("ì´ì§€í˜œ, ìµœì§€ì˜, ê¶Œì†Œí¬, ë¬¸ì¢…í˜„, êµ¬ìí˜„, ê¹€ì˜ì¤€")

st.write('---')

# Explanation
row1_spacer1, row1_1, row1_spacer2 = st.columns((0.01, 0.01, 0.01))

with row1_1:
    st.markdown("### ní–‰ì‹œ ê°€ì´ë“œë¼ì¸")
    st.markdown("1. í•˜ë‹¨ì— ìˆëŠ” í…ìŠ¤íŠ¸ë°”ì— 5ì ì´í•˜ ë‹¨ì–´ë¥¼ ë„£ì–´ì£¼ì„¸ìš”")
    st.markdown("2. 'ní–‰ì‹œ ì œì‘í•˜ê¸°' ë²„íŠ¼ì„ í´ë¦­í•´ì£¼ì„¸ìš”")

st.write('---')

# Model & Input
row2_spacer1, row2_1, row2_spacer2= st.columns((0.01, 0.01, 0.01))

# Word Input
with row2_1:
    word_input = st.text_input(
            "ní–‰ì‹œì— ì‚¬ìš©í•  ë‹¨ì–´ë¥¼ ì ê³  ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.(ìµœëŒ€ 5ì) ğŸ‘‡",
            placeholder='í•œê¸€ ë‹¨ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”',
            max_chars=5
    )
        
    if st.button('ní–‰ì‹œ ì œì‘í•˜ê¸°'):
        st.write("ní–‰ì‹œ ë‹¨ì–´ :  ", word_input)
        with st.spinner('ì ì‹œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...'):
            result = n_line_poem(word_input)
        st.success('ì™„ë£ŒëìŠµë‹ˆë‹¤!')
        for r in result:
            st.write(f'{r} : {result[r]}')



