import requests
import streamlit as st
from streamlit_lottie import st_lottie
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Page Config
st.set_page_config(
    page_title="ë…¸ë˜ ê°€ì‚¬ ní–‰ì‹œ",
    page_icon="ğŸ’Œ",
    layout="wide"
)

### Model
tokenizer = AutoTokenizer.from_pretrained("wumusill/final_project_kogpt2")

@st.cache(show_spinner=False)
def load_model():
    model = AutoModelForCausalLM.from_pretrained("wumusill/final_project_kogpt2")
    return model

model = load_model()

# Class : Dict ì¤‘ë³µ í‚¤ ì¶œë ¥
class poem(object):
    def __init__(self,letter):
        self.letter = letter

    def __str__(self):
        return self.letter

    def __repr__(self):
        return "'"+self.letter+"'"


def n_line_poem(input_letter):

    # ë‘ìŒ ë²•ì¹™ ì‚¬ì „
    dooeum = {"ë¼":"ë‚˜", "ë½":"ë‚™", "ë€":"ë‚œ", "ë„":"ë‚ ", "ëŒ":"ë‚¨", "ë":"ë‚©", "ë‘":"ë‚­", 
          "ë˜":"ë‚´", "ë­":"ëƒ‰", "ëƒ‘":"ì•½", "ëµ":"ì•½", "ëƒ¥":"ì–‘", "ëŸ‰":"ì–‘", "ë…€":"ì—¬", 
          "ë ¤":"ì—¬", "ë…":"ì—­", "ë ¥":"ì—­", "ë…„":"ì—°", "ë ¨":"ì—°", "ë…ˆ":"ì—´", "ë ¬":"ì—´", 
          "ë…":"ì—¼", "ë ´":"ì—¼", "ë µ":"ì—½", "ë…•":"ì˜", "ë ¹":"ì˜", "ë…œ":"ì˜ˆ", "ë¡€":"ì˜ˆ", 
          "ë¡œ":"ë…¸", "ë¡":"ë…¹", "ë¡ ":"ë…¼", "ë¡±":"ë†", "ë¢°":"ë‡Œ", "ë‡¨":"ìš”", "ë£Œ":"ìš”", 
          "ë£¡":"ìš©", "ë£¨":"ëˆ„", "ë‰´":"ìœ ", "ë¥˜":"ìœ ", "ë‰µ":"ìœ¡", "ë¥™":"ìœ¡", "ë¥œ":"ìœ¤", 
          "ë¥ ":"ìœ¨", "ë¥­":"ìœµ", "ë¥µ":"ëŠ‘", "ë¦„":"ëŠ ", "ë¦‰":"ëŠ¥", "ë‹ˆ":"ì´", "ë¦¬":"ì´", 
          "ë¦°":'ì¸', 'ë¦¼':'ì„', 'ë¦½':'ì…'}
    # ê²°ê³¼ë¬¼ì„ ë‹´ì„ list
    res_l = []

    # í•œ ê¸€ìì”© ì¸ë±ìŠ¤ì™€ í•¨ê»˜ ê°€ì ¸ì˜´
    for idx, val in enumerate(input_letter):
        # ë‘ìŒ ë²•ì¹™ ì ìš©
        if val in dooeum.keys():
            val = dooeum[val]
        
        times = 0
        while times < 3:
            # ë§Œì•½ idx ê°€ 0 ì´ë¼ë©´ == ì²« ê¸€ì
            if idx == 0:
                # ì²« ê¸€ì ì¸ì½”ë”©
                input_ids = tokenizer.encode(
                val, add_special_tokens=False, return_tensors="pt")

                # ì²« ê¸€ì ì¸ì½”ë”© ê°’ìœ¼ë¡œ ë¬¸ì¥ ìƒì„±
                output_sequence = model.generate(
                    input_ids=input_ids, 
                    do_sample=True, max_length=42, no_repeat_ngram_size=2,
                    min_length=5, temperature=0.9, repetition_penalty=1.5)

            # ì²« ê¸€ìê°€ ì•„ë‹ˆë¼ë©´
            else:
                # ì¢€ë” ë§¤ë„ëŸ¬ìš´ ì‚¼í–‰ì‹œë¥¼ ìœ„í•´ ì´ì „ ë¬¸ì¥ì´ë‘ í˜„ì¬ ìŒì ˆ ì—°ê²°
                # ì´í›„ generate ëœ ë¬¸ì¥ì—ì„œ ì´ì „ ë¬¸ì¥ì— ëŒ€í•œ ë°ì´í„° ì œê±°
                link_with_pre_sentence = " ".join(res_l) + " " + val  
                # print(link_with_pre_sentence)

                # ì—°ê²°ëœ ë¬¸ì¥ì„ ì¸ì½”ë”©
                input_ids = tokenizer.encode(
                link_with_pre_sentence, add_special_tokens=False, return_tensors="pt")

                # ì¸ì½”ë”© ê°’ìœ¼ë¡œ ë¬¸ì¥ ìƒì„±
                output_sequence = model.generate(
                    input_ids=input_ids, 
                    do_sample=True, max_length=42, no_repeat_ngram_size=2,
                    min_length=len_sequence, temperature=0.9, repetition_penalty=1.5)

            # ìƒì„±ëœ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì¸ì½”ë”© ë˜ì–´ìˆê³ , ìƒì„±ëœ ë¬¸ì¥ ë’¤ë¡œ padding ì´ ìˆëŠ” ìƒíƒœ)
            generated_sequence = output_sequence.tolist()[0]

            # padding index ì•ê¹Œì§€ slicing í•¨ìœ¼ë¡œì¨ padding ì œê±°, paddingì´ ì—†ì„ ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì— ì¡°ê±´ë¬¸ í™•ì¸ í›„ ì œê±°
            if tokenizer.pad_token_id in generated_sequence:
                generated_sequence = generated_sequence[:generated_sequence.index(tokenizer.pad_token_id)]

            # ì²« ê¸€ìê°€ ì•„ë‹ˆë¼ë©´, generate ëœ ìŒì ˆë§Œ ê²°ê³¼ë¬¼ listì— ë“¤ì–´ê°ˆ ìˆ˜ ìˆê²Œ ì• ë¬¸ì¥ì— ëŒ€í•œ ì¸ì½”ë”© ê°’ ì œê±°
            # print(generated_sequence)
            if idx != 0:
                # ì´ì „ ë¬¸ì¥ì˜ ê¸¸ì´ ì´í›„ë¡œ ìŠ¬ë¼ì´ì‹±í•´ì„œ ì• ë¬¸ì¥ ì œê±°
                generated_sequence = generated_sequence[len_sequence:]

                # ë‹¤ìŒ ìŒì ˆì„ ìœ„í•´ ê¸¸ì´ ê°±ì‹ 
                len_sequence += len(generated_sequence)        

            # ì²« ê¸€ìë¼ë©´
            else:
                # ì‹œí€€ìŠ¤ ê¸¸ì´ ì €ì¥
                len_sequence = len(generated_sequence)

            # print(last_sequence)

            # ê²°ê³¼ë¬¼ ë””ì½”ë”©
            decoded_sequence = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)
            
            if len(decoded_sequence) > 1:
                break
            else:
                times += 1
                continue
                
        # ê²°ê³¼ë¬¼ ë¦¬ìŠ¤íŠ¸ì— ë‹´ê¸°
        res_l.append(decoded_sequence)

    poem_dict = {}

    for letter, res in zip(input_letter, res_l):
        poem_dict[poem(letter)] = res

    return poem_dict

###

# Image(.gif)
@st.cache(show_spinner=False)
def load_lottieurl(url: str):
    r = requests.get(url)
    if r.status_code != 200:
        return None
    return r.json()

lottie_url = "https://assets7.lottiefiles.com/private_files/lf30_fjln45y5.json"

lottie_json = load_lottieurl(lottie_url)
st_lottie(lottie_json, speed=1, height=200, key="initial")


# Title
row0_spacer1, row0_1, row0_spacer2, row0_2, row0_spacer3 = st.columns(
    (0.01, 2, 0.05, 1, 0.01)
)

with row0_1:
    st.title("í•œê¸€ ë…¸ë˜ ê°€ì‚¬ ní–‰ì‹œâœ")
    st.subheader("ğŸ¦ë©‹ìŸì´ì‚¬ìì²˜ëŸ¼ AIS7 íŒŒì´ë„ í”„ë¡œì íŠ¸")

with row0_2:
    st.write("")
    st.subheader(
        "í•´íŒŒë¦¬íŒ€"
    )
    st.write("ì´ì§€í˜œ, ìµœì§€ì˜, ê¶Œì†Œí¬")
    st.write("ë¬¸ì¢…í˜„, êµ¬ìí˜„, ê¹€ì˜ì¤€")

st.write('---')

# Explanation
row1_spacer1, row1_1, row1_spacer2 = st.columns((0.01, 3, 0.01))

with row1_1:
    st.markdown("### ê°€ì´ë“œë¼ì¸")
    st.markdown("1. í•˜ë‹¨ì— ìˆëŠ” í…ìŠ¤íŠ¸ë°”ì— ë‹¨ì–´ë¥¼ ë„£ì–´ì£¼ì„¸ìš”")
    st.markdown("2. 'ní–‰ì‹œ ì œì‘í•˜ê¸°' ë²„íŠ¼ì„ í´ë¦­í•´ì£¼ì„¸ìš”")

st.write('---')

# Model & Input
row2_spacer1, row2_1, row2_spacer2, row2_2, row2_spacer3, row2_3, row2_spacer4 = st.columns((0.01, 1.5, 0.05, 1.5, 0.05, 1.5, 0.05))

# Word Input
if "generate" not in st.session_state:
    st.session_state.generate = False

with row2_2:
    word_input = st.text_input(
            "ní–‰ì‹œì— ì‚¬ìš©í•  ë‹¨ì–´ë¥¼ ì ê³  Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”. ğŸ‘‡",
            placeholder='í•œê¸€ ë‹¨ì–´',
            max_chars=10
    )
    
    if word_input:
        st.write("ní–‰ì‹œ ë‹¨ì–´ :  ", word_input)

    if st.button('ní–‰ì‹œ ì œì‘í•˜ê¸°'):
        with st.spinner('ì ì‹œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...'):
            result = n_line_poem(word_input)
        st.success('ì™„ë£ŒëìŠµë‹ˆë‹¤!')
        for r in result:
            st.write(f'{r} : {result[r]}')



